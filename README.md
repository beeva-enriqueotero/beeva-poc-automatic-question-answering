# beeva-poc-automatic-question-answering
Proof of Concept with Automatic Question Answering and Question Generation

## Question Generation
### Experiment 1: Use NLP to generate "fill the gap" questionnaires from Wikipedia.

NLP Tools: *Python3, NLTK, TextBlob, (Multilingual) WordNet*
Process
- Segment into sentences with TextBlob Tokenizer
- Filter sentences with simple rules
- Select a common noun per sentence with Part of Speech (PoS) Tagger
  - **English**: *TextBlob PatternParser* (Penn Treebank tagset)
  - **Spanish**: *spaghetti-tagger* (EAGLES tagset). *Note*: Pattern supports Spanish, but not python3
- Generate distractors with *WordNet*

* See https://github.com/beeva-enriqueotero/wikipedia-question-generator
* Based on https://github.com/atbaker/wikipedia-question-generator


### Old dummy version
```
python mvp.py
# Usage: python mvp.py <word_to_search_in_wikipedia> <number_of_sentences>
python mvp.py Isabel 1
# Output: This set of names is a southwestern European variant of the ... name Elisheva, also represented in English and other western languages as Elizabeth
# Missing word: Hebrew
```

## Question Answering

### Experiment 2: Fill The Gaps 
Answer fill-the-gap questions on multi-choice tests. Same as generated by experiment 1. In order to filter easy questions.
* a) With *Word Embeddings*: using [gensim's `predict_output_word` implementation](https://github.com/RaRe-Technologies/gensim/blob/4a3b2137ce3c2c8fc83a7e8e0921991e1862e9c4/gensim/models/word2vec.py#L1286). Released on v2.1.0 (2017/05/12)
* b) With *Ngrams*: using (English) 2-grams from 2009 Google collection. See [notebook](ngrams_demo.ipynb) for details.

#### Results:
* a) Fail. Not better than random guess.
* b) Accuracy: between 49% and 55%.  

#### Conclusions:
* Word2vec alternative fails. Not better than random guess.
* Ngrams (2grams) get much better accuracy than random guess.

### Experiment 3: Test MS-Cognitive https://qnamaker.ai/: From FAQ to Bot in minutes
* Example [Spanish FAQs](https://aws.amazon.com/es/ec2/faqs/)
* Example [questions](https://github.com/beeva-enriqueotero/beeva-poc-automatic-question-answering/blob/master/data/test_qnamaker_spanish.txt)

#### Results:
* 17/40 right 
* 11/40 related
* 30% fails!

#### Conclusions:
* qnamaker.ai is not state-of-the-art technology, despite it is beta.
* qnamaker.ai is based on "index & rank". Old techniques from Information Retrieval in 70s. Similar to Solr or ElasticSearch
